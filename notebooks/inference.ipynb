{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_DIR = '../data'\n",
    "IMAGES_DIR = os.path.join(DATA_DIR, 'Images')\n",
    "CAPTIONS_FILE = os.path.join(DATA_DIR, 'captions.txt')\n",
    "MODELS_DIR = '../models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture (same as training)\n",
    "class EncoderCNN(nn.Module):\n",
    "    \"\"\"CNN Encoder using pre-trained ResNet50.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        \n",
    "        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.fc = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.fc(features)\n",
    "        features = self.bn(features)\n",
    "        return features\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"LSTM Decoder for caption generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=2, dropout=0.5):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, \n",
    "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embedding(captions[:, :-1])\n",
    "        features = features.unsqueeze(1)\n",
    "        embeddings = torch.cat([features, embeddings], dim=1)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        outputs = self.fc(lstm_out)\n",
    "        return outputs\n",
    "    \n",
    "    def generate(self, features, max_length=30, temperature=1.0):\n",
    "        \"\"\"Generate caption for inference.\"\"\"\n",
    "        generated = []\n",
    "        h = torch.zeros(self.num_layers, 1, self.hidden_size).to(features.device)\n",
    "        c = torch.zeros(self.num_layers, 1, self.hidden_size).to(features.device)\n",
    "        x = features.unsqueeze(1)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            lstm_out, (h, c) = self.lstm(x, (h, c))\n",
    "            output = self.fc(lstm_out.squeeze(1))\n",
    "            output = output / temperature\n",
    "            predicted = output.argmax(dim=1)\n",
    "            generated.append(predicted.item())\n",
    "            \n",
    "            if predicted.item() == 2:  # <END> token\n",
    "                break\n",
    "            \n",
    "            x = self.embedding(predicted).unsqueeze(1)\n",
    "        \n",
    "        return generated\n",
    "\n",
    "\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    \"\"\"Complete Image Captioning Model.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=2):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.encoder = EncoderCNN(embed_size)\n",
    "        self.decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "        \n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs\n",
    "    \n",
    "    def generate_caption_indices(self, image, max_length=30, temperature=1.0):\n",
    "        \"\"\"Generate caption indices for a single image.\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            features = self.encoder(image)\n",
    "            caption_indices = self.decoder.generate(features, max_length, temperature)\n",
    "        return caption_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary class for decoding\n",
    "class Vocabulary:\n",
    "    \"\"\"Vocabulary class for mapping words to indices and vice versa.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.pad_token = '<PAD>'\n",
    "        self.start_token = '<START>'\n",
    "        self.end_token = '<END>'\n",
    "        self.unk_token = '<UNK>'\n",
    "    \n",
    "    def load(self, vocab_data):\n",
    "        \"\"\"Load vocabulary from saved data.\"\"\"\n",
    "        self.word2idx = vocab_data['word2idx']\n",
    "        self.idx2word = {int(k): v for k, v in vocab_data['idx2word'].items()}\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        \"\"\"Convert list of indices back to caption string.\"\"\"\n",
    "        words = []\n",
    "        for idx in indices:\n",
    "            word = self.idx2word.get(idx, self.unk_token)\n",
    "            if word == self.end_token:\n",
    "                break\n",
    "            if word not in [self.start_token, self.pad_token]:\n",
    "                words.append(word)\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary\n",
    "with open(os.path.join(MODELS_DIR, 'vocab.pkl'), 'rb') as f:\n",
    "    vocab_data = pickle.load(f)\n",
    "\n",
    "vocab = Vocabulary()\n",
    "vocab.load(vocab_data)\n",
    "print(f'Vocabulary loaded: {len(vocab)} words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "checkpoint = torch.load(os.path.join(MODELS_DIR, 'best_model.pth'), map_location=device)\n",
    "\n",
    "# Get model parameters from final_model.pth which has the config\n",
    "model_config = torch.load(os.path.join(MODELS_DIR, 'final_model.pth'), map_location=device)\n",
    "\n",
    "model = ImageCaptioningModel(\n",
    "    embed_size=model_config['embed_size'],\n",
    "    hidden_size=model_config['hidden_size'],\n",
    "    vocab_size=model_config['vocab_size'],\n",
    "    num_layers=model_config['num_layers']\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded from epoch {checkpoint['epoch']} with val_loss: {checkpoint['val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transform (same as training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Caption Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_path: str, model: any) -> str:\n",
    "    \"\"\"\n",
    "    Takes a path to an image and returns a generated caption string.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        model: The trained ImageCaptioningModel\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated caption for the image\n",
    "    \"\"\"\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate caption indices\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        caption_indices = model.generate_caption_indices(image_tensor)\n",
    "    \n",
    "    # Decode to string\n",
    "    caption = vocab.decode(caption_indices)\n",
    "    \n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function\n",
    "test_images = os.listdir(IMAGES_DIR)[:3]\n",
    "for img_name in test_images:\n",
    "    img_path = os.path.join(IMAGES_DIR, img_name)\n",
    "    caption = generate_caption(img_path, model)\n",
    "    print(f'{img_name}: {caption}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation images and ground truth captions\n",
    "with open(os.path.join(MODELS_DIR, 'val_images.pkl'), 'rb') as f:\n",
    "    val_images = pickle.load(f)\n",
    "\n",
    "# Load captions dataframe for ground truth\n",
    "df = pd.read_csv(CAPTIONS_FILE)\n",
    "\n",
    "print(f'Validation images: {len(val_images)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_prediction(image_name, show_all_captions=True):\n",
    "    \"\"\"\n",
    "    Display an image with its generated caption and ground truth captions.\n",
    "    \"\"\"\n",
    "    img_path = os.path.join(IMAGES_DIR, image_name)\n",
    "    \n",
    "    # Generate caption\n",
    "    generated = generate_caption(img_path, model)\n",
    "    \n",
    "    # Get ground truth captions\n",
    "    ground_truth = df[df['image'] == image_name]['caption'].tolist()\n",
    "    \n",
    "    # Display\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    image = Image.open(img_path)\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    title = f'Generated: {generated}\\n\\n'\n",
    "    if show_all_captions:\n",
    "        title += 'Ground Truth:\\n'\n",
    "        for i, gt in enumerate(ground_truth, 1):\n",
    "            title += f'{i}. {gt}\\n'\n",
    "    else:\n",
    "        title += f'Ground Truth: {ground_truth[0]}'\n",
    "    \n",
    "    ax.set_title(title, fontsize=10, wrap=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return generated, ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate on random validation images\n",
    "np.random.seed(42)\n",
    "sample_images = np.random.choice(val_images, size=10, replace=False)\n",
    "\n",
    "results = []\n",
    "for img_name in sample_images:\n",
    "    gen, gt = display_prediction(img_name)\n",
    "    results.append({'image': img_name, 'generated': gen, 'ground_truth': gt})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analysis\n",
    "\n",
    "### Successful Captions\n",
    "\n",
    "We analyze cases where the model generates semantically accurate captions that capture the main subjects and actions in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_word_overlap(generated, ground_truths):\n",
    "    \"\"\"\n",
    "    Compute the word overlap between generated caption and ground truths.\n",
    "    Returns the best overlap ratio among all ground truth captions.\n",
    "    \"\"\"\n",
    "    gen_words = set(generated.lower().split())\n",
    "    \n",
    "    best_overlap = 0\n",
    "    for gt in ground_truths:\n",
    "        gt_words = set(gt.lower().split())\n",
    "        if len(gen_words) > 0:\n",
    "            overlap = len(gen_words & gt_words) / len(gen_words)\n",
    "            best_overlap = max(best_overlap, overlap)\n",
    "    \n",
    "    return best_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all validation images and find successes/failures\n",
    "all_results = []\n",
    "\n",
    "print('Evaluating validation set...')\n",
    "for img_name in val_images[:100]:  # Evaluate first 100 for speed\n",
    "    img_path = os.path.join(IMAGES_DIR, img_name)\n",
    "    generated = generate_caption(img_path, model)\n",
    "    ground_truth = df[df['image'] == img_name]['caption'].tolist()\n",
    "    overlap = compute_word_overlap(generated, ground_truth)\n",
    "    \n",
    "    all_results.append({\n",
    "        'image': img_name,\n",
    "        'generated': generated,\n",
    "        'ground_truth': ground_truth,\n",
    "        'overlap': overlap\n",
    "    })\n",
    "\n",
    "# Sort by overlap score\n",
    "all_results_sorted = sorted(all_results, key=lambda x: x['overlap'], reverse=True)\n",
    "\n",
    "print(f'Average word overlap: {np.mean([r[\"overlap\"] for r in all_results]):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display successful captions (high overlap)\n",
    "print('=== SUCCESSFUL CAPTIONS (High Overlap) ===')\n",
    "print()\n",
    "\n",
    "for result in all_results_sorted[:5]:\n",
    "    print(f\"Image: {result['image']}\")\n",
    "    print(f\"Generated: {result['generated']}\")\n",
    "    print(f\"Ground Truth (1 of {len(result['ground_truth'])}): {result['ground_truth'][0]}\")\n",
    "    print(f\"Word Overlap: {result['overlap']:.2%}\")\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top 3 successful predictions\n",
    "print('Top 3 Successful Predictions:')\n",
    "for result in all_results_sorted[:3]:\n",
    "    display_prediction(result['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failure Cases\n",
    "\n",
    "We analyze cases where the model generates captions that don't accurately describe the image content. Common failure modes include:\n",
    "- Hallucinating objects not present in the image\n",
    "- Missing key subjects or actions\n",
    "- Generating generic or repetitive descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display failure cases (low overlap)\n",
    "print('=== FAILURE CASES (Low Overlap) ===')\n",
    "print()\n",
    "\n",
    "for result in all_results_sorted[-5:]:\n",
    "    print(f\"Image: {result['image']}\")\n",
    "    print(f\"Generated: {result['generated']}\")\n",
    "    print(f\"Ground Truth (1 of {len(result['ground_truth'])}): {result['ground_truth'][0]}\")\n",
    "    print(f\"Word Overlap: {result['overlap']:.2%}\")\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bottom 3 (failure) predictions\n",
    "print('Bottom 3 Failure Cases:')\n",
    "for result in all_results_sorted[-3:]:\n",
    "    display_prediction(result['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis summary\n",
    "overlaps = [r['overlap'] for r in all_results]\n",
    "\n",
    "print('=== SUMMARY ===' )\n",
    "print(f'Total images evaluated: {len(all_results)}')\n",
    "print(f'Average word overlap: {np.mean(overlaps):.2%}')\n",
    "print(f'Median word overlap: {np.median(overlaps):.2%}')\n",
    "print(f'High quality (>50% overlap): {sum(1 for o in overlaps if o > 0.5)} images')\n",
    "print(f'Medium quality (25-50% overlap): {sum(1 for o in overlaps if 0.25 <= o <= 0.5)} images')\n",
    "print(f'Low quality (<25% overlap): {sum(1 for o in overlaps if o < 0.25)} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot overlap distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(overlaps, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Word Overlap Score')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.title('Distribution of Caption Quality (Word Overlap with Ground Truth)')\n",
    "plt.axvline(np.mean(overlaps), color='red', linestyle='--', label=f'Mean: {np.mean(overlaps):.2%}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Failure Patterns\n",
    "\n",
    "Based on the analysis above, common failure patterns include:\n",
    "\n",
    "1. **Object Misidentification**: The model sometimes confuses similar objects (e.g., dog vs. cat)\n",
    "2. **Action Confusion**: The model may describe the wrong action taking place\n",
    "3. **Generic Captions**: In ambiguous images, the model may fall back to generic descriptions\n",
    "4. **Missing Context**: Important contextual elements like settings or backgrounds may be ignored\n",
    "\n",
    "These issues are typical for image captioning models and can be improved with:\n",
    "- More training data\n",
    "- Attention mechanisms\n",
    "- Larger encoder networks\n",
    "- Beam search decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final demonstration: caption any image\n",
    "def caption_image(image_path):\n",
    "    \"\"\"\n",
    "    Generate and display caption for any image.\n",
    "    \"\"\"\n",
    "    caption = generate_caption(image_path, model)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    image = Image.open(image_path)\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Caption: {caption}', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return caption\n",
    "\n",
    "# Example usage:\n",
    "# caption_image('path/to/your/image.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
